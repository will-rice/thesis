\chapter*{LIST OF SYMBOLS}
\vspace{\baselineskip}
\begin{flushleft}
$\Sigma$, Summation\\
\vspace{\baselineskip}
$P$, Probability of event\\
\vspace{\baselineskip}
$f$, Function that accepts arguments\\
\vspace{\baselineskip}
$\lambda$, Penalty term for loss function\\
\vspace{\baselineskip}
$KL(P||Q)$, KL divergence of P and Q\\
\vspace{\baselineskip}
$W(p_r, p_\theta)$, Wasserstein distance between the prior and posterior distribution\\
\vspace{\baselineskip}
$\alpha$, learning rate\\
\vspace{\baselineskip}
$\mathbb{E}$, Expected\\
\vspace{\baselineskip}
$\hat{\theta}$, output vector\\
\vspace{\baselineskip}
$\sup$, suprema \\
\vspace{\baselineskip}
$\max_{w \in W}$, maximum of w in W\\
\vspace{\baselineskip}
$\mathbb{E}_{x \sim p_r}[f_w(x)]$, expected distribution of function when input is x\\
\vspace{\baselineskip}
$\lambda(\|{\nabla_{\hat{x}} D_{w}(\hat{x})}\|_{2} - 1)^2$, gradient penalty term\\
\vspace{\baselineskip}
$\phi$, frequency transformation\\
\vspace{\baselineskip}
$L^{(i)}$, Loss at iteration i\\
\vspace{\baselineskip}
$z$, latent distribution\\
\vspace{\baselineskip}
$x \backsim \mathbb{P} _{r}$, sample x that has probability distribution $\mathbb{P}_r$\\
\vspace{\baselineskip}
$D_w$, discriminator weight\\
\vspace{\baselineskip}
$\hat{y}$, predicted value\\
\vspace{\baselineskip}
$L_2^{(i)}$, second loss term at iteration i\\
\vspace{\baselineskip}
$G(z)$, generator output\\
\vspace{\baselineskip}
$\nabla$, gradient\\
\vspace{\baselineskip}
$n$, batch size
\end{flushleft}